{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCm_vS8KwM6y"
      },
      "source": [
        "# ECE 494 Intro To Cloud Computing Final Project\r\n",
        "\r\n",
        "This notebook contains our code for the final project. It is nearly identical to the one used for Amazon EMR. The one for Amazon EMR is sligtly modified to fit that environment.\r\n",
        "\r\n",
        "\r\n",
        "This notebook follows the same sequence presented in the final presentation.<br>\r\n",
        "\r\n",
        "Gather Data -> PreProcess -> Feature Extraction -> ML Algorithm -> Results\r\n",
        "\r\n",
        "\r\n",
        "Each of the above steps are defined as methods and then a single driver calls them all. Each of their section has a title matching their functions.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "To run this code the following must be done:\r\n",
        "- Put the file name \"english\" in the same directory as this code or update the PATH_TO_ENGLISH constant with the path leading to the file.\r\n",
        "- Know the path to the input files.\r\n",
        "- Run all of the code up to and including the diver method definition.\r\n",
        "- Call the driver method clusteringPipeline(inputPath) with the path to the input file. The input file is a text file that will be analyzed.\r\n",
        "\r\n",
        "\r\n",
        "Notes:\r\n",
        "  - Gather Data in this notebook refers to simply reading the text file with input. The twitter api code is in a separate file.\r\n",
        "  - Results in this notebook refers to presenting the words that serve as topic cluster centers. The actual analysis is in the presentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAa8AcPEJxjf"
      },
      "source": [
        "#!cat /proc/cpuinfo #run to see cpu info for colab; req for comparison with aws emr; extaneous and not necessary for the rest of the code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL8rVZVEeC_F"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5-6TKQzS4zn"
      },
      "source": [
        "#pyspark:\r\n",
        "!pip install pyspark\r\n",
        "import pyspark as spark\r\n",
        "from pyspark import SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer,CountVectorizer\r\n",
        "from pyspark.ml.clustering import KMeans\r\n",
        "from pyspark.mllib.clustering import KMeans as KMeans_mllib\r\n",
        "\r\n",
        "\r\n",
        "#nltk imports\r\n",
        "import nltk\r\n",
        "from nltk.tokenize.casual import TweetTokenizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yidymfXHrO72"
      },
      "source": [
        "## Downloads\r\n",
        "\r\n",
        "To be Downloaded:\r\n",
        "  - Twitter Dataset\r\n",
        "  - Stopwords\r\n",
        "\r\n",
        "This is a run once only step to get the labeled twitter data and stopwords.\r\n",
        "\r\n",
        "\r\n",
        "After this manual processing is done on the json file to convert to \r\n",
        ".txt files that can be read and processed by pyspark.\r\n",
        "\r\n",
        "We will include the downloaded files in the zip folder for the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FeWTh1BuRxW5",
        "outputId": "0b8f14c1-3bb8-451b-c8c5-ae278a877751"
      },
      "source": [
        "r\"\"\"\r\n",
        "twitter_samples = nltk.download('twitter_samples',download_dir='./')\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ntwitter_samples = nltk.download('twitter_samples',download_dir='./')\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNJXm7qqy_ws"
      },
      "source": [
        "#nltk.download('stopwords',download_dir='/root/nltk_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yznJ6xU1AoU0"
      },
      "source": [
        "## Spark Starter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4tSxazuOSgz"
      },
      "source": [
        "#spark context starter:\r\n",
        "sc = SparkContext(appName=\"inModuleTask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhVSJKqf_1pA"
      },
      "source": [
        "# Declaration/Initializations\r\n",
        "\r\n",
        "Below all of the modules,constants, and variables to be used <br>\r\n",
        "in a global manner across the code will be either declared or <br>\r\n",
        "initiated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLg2bxOpzmZi"
      },
      "source": [
        "#dataset variables and constants:\r\n",
        "# - path constants:\r\n",
        "POSITIVE_TWEETS_PATH = \"./positive_tweets_short.txt\"\r\n",
        "NEGATIVE_TWEETS_PATH = \"./negative_tweets_short.txt\"\r\n",
        "PATH_TO_ENGLISH = \"./english\"\r\n",
        "\r\n",
        "\r\n",
        "# modules that will be used by the pre-processor\r\n",
        "tokenizer_tweets = TweetTokenizer(reduce_len=True,strip_handles=True)\r\n",
        "with open(PATH_TO_ENGLISH) as eng:\r\n",
        "  engSet = set(eng)\r\n",
        "engSet = set(map(lambda x: x[0:len(x)-1],engSet))\r\n",
        "stp_words = engSet\r\n",
        "porter = PorterStemmer()\r\n",
        "\r\n",
        "\r\n",
        "# for feature extraction:\r\n",
        "NUM_FEATURES = 1<<16 #this is the same as 2^N when 1<<N\r\n",
        "NUM_WORDS_KEEP = 3\r\n",
        "\r\n",
        "#clustering vars/constants:\r\n",
        "NUM_CLUSTERS = 5\r\n",
        "MAX_ITERATION = 30\r\n",
        "NUM_TOPICS_PER_CLUSTER = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVVk2t-TZaii"
      },
      "source": [
        "#print(stp_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJnap7trWRPr"
      },
      "source": [
        "# Gather Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMqrqB1wXTsC"
      },
      "source": [
        "def getTextData(path):\r\n",
        "  r\"\"\"\r\n",
        "    Will get the (text) file at the given path and return\r\n",
        "    an rdd.\r\n",
        "  \r\n",
        "  @param:\r\n",
        "    - path : path to the file\r\n",
        "  \r\n",
        "  @return:\r\n",
        "    - rdd : rdd of the textfile; separated by lines\r\n",
        "  \"\"\"\r\n",
        "  dataRdd = sc.textFile(path)\r\n",
        "  return dataRdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-o5G3UDOOIM"
      },
      "source": [
        "# Pre-Processing\r\n",
        "\r\n",
        "Steps in pre-processing:\r\n",
        "  - For each line of text (= one tweet):\r\n",
        "    - get rid of all non alphabetical characters\r\n",
        "    - lowercase all words\r\n",
        "    - get rid of stop words\r\n",
        "  - Break each line into words\r\n",
        "  - Convert each word to its root form, i.e stemming (through nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ7HPaMb_aWp"
      },
      "source": [
        "### Pre-Processing Method\r\n",
        "\r\n",
        "Will be plugged into pyspark map function for both\r\n",
        "positive and negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r52elpuWSzFh"
      },
      "source": [
        "#pre-processor method, will be the map function for pyspark:\r\n",
        "def preProcessTweet(tweet):\r\n",
        "  r\"\"\"\r\n",
        "    Does pre-processing of each tweet and returns a list\r\n",
        "    of processed words.\r\n",
        "\r\n",
        "    NOTE: Each line in input doc has to be a tweet for this to work.\r\n",
        "\r\n",
        "    Steps taken by this method:\r\n",
        "      - Retains only words in the tweets.\r\n",
        "      - Gets rid of stop words\r\n",
        "      - Stems each word using nltk library functions\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  #strip begining and ending whitespaces:\r\n",
        "  tweet = tweet.strip()\r\n",
        "\r\n",
        "  #tokenize the words based on nltk twitter tokenization:\r\n",
        "  tokenized_tweet = tokenizer_tweets.tokenize(tweet)\r\n",
        "\r\n",
        "  #keep only the words:\r\n",
        "  wordRegex = r'^[a-zA-Z].*$'\r\n",
        "  tokenized_tweet_onlyWords = []\r\n",
        "  for s in tokenized_tweet:\r\n",
        "    result = re.search(wordRegex,s)\r\n",
        "    if result is not None:\r\n",
        "      tokenized_tweet_onlyWords.append(result.string.lower())\r\n",
        "  \r\n",
        "  #get rid of links (NOTE: this is an approximation and may not cover everything):\r\n",
        "  tokenized_tweet_onlyWords_noLinks = []\r\n",
        "  linkRemovalRegex = r\"^https?:.*$|^.*\\..*$|^.*[@\\.].*$\"\r\n",
        "  for s in tokenized_tweet_onlyWords:\r\n",
        "    result = re.search(linkRemovalRegex,s)\r\n",
        "    if result is None:\r\n",
        "      tokenized_tweet_onlyWords_noLinks.append(s)\r\n",
        "\r\n",
        "  #get rid of stopwords:\r\n",
        "  tokenized_tweet_onlyWords_noStopwords = []\r\n",
        "  for word in tokenized_tweet_onlyWords_noLinks:\r\n",
        "    if word not in stp_words:\r\n",
        "      tokenized_tweet_onlyWords_noStopwords.append(word)\r\n",
        "  \r\n",
        "  #stem the word:\r\n",
        "  tokenizedTweet_onlyWords_noStopWords_stemmed = []\r\n",
        "  for word in tokenized_tweet_onlyWords_noStopwords:\r\n",
        "    tokenizedTweet_onlyWords_noStopWords_stemmed.append(porter.stem(word))\r\n",
        "  \r\n",
        "  \r\n",
        "  toBeReturned = tokenizedTweet_onlyWords_noStopWords_stemmed\r\n",
        "\r\n",
        "  #change all remaining words to lowercase:\r\n",
        "  for i in range(len(toBeReturned)):\r\n",
        "    toBeReturned[i] = toBeReturned[i].lower()\r\n",
        "\r\n",
        "  return toBeReturned \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNNkBpNLcJE3"
      },
      "source": [
        "## Pre-Processing Abstraction\r\n",
        "\r\n",
        "A method that calls the pre-processor and returns\r\n",
        "the pre-processed rdd. This is only for the sake of cleaner code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5JBBSe1cToI"
      },
      "source": [
        "def doDataPreProcessing(dataRdd,numPartitions):\r\n",
        "  r\"\"\"\r\n",
        "    Given the rdd of a textfile, this method will\r\n",
        "    return an rdd with pre-processed data. The\r\n",
        "    pre-processing will take place using pyspark map function with \r\n",
        "    the preProcessTweet() method.\r\n",
        "\r\n",
        "  @param:\r\n",
        "    - dataRdd: rdd that contains the entire text data. This is the\r\n",
        "               rdd generated by getTextData().\r\n",
        "  \r\n",
        "  @return:\r\n",
        "    - processedRdd: rdd that contains processed tweets. The format for\r\n",
        "                    each entry is a list of words relevant for each tweet.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  if numPartitions is None:\r\n",
        "    return dataRdd.map(preProcessTweet)\r\n",
        "  \r\n",
        "  return dataRdd.map(preProcessTweet)\\\r\n",
        "                .partitionBy(numPartitions)\\\r\n",
        "                .persist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TMCzAXR-l0o"
      },
      "source": [
        "# Feature Extraction\r\n",
        "\r\n",
        "Generate Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYa3ev3y-lTh"
      },
      "source": [
        "\r\n",
        "def generateWordDict(table):\r\n",
        "  r\"\"\"\r\n",
        "    Creates a dictionary where the key values are indices and the \r\n",
        "    values are words that correspond to said indices. The indices are\r\n",
        "    obatained from PySpark's HashingTF & IDF modules and correspond to \r\n",
        "    words in the document.\r\n",
        "\r\n",
        "    This dictionary is created so we can interpret the results after KMeans\r\n",
        "    runs. KMeans returns a vector of numbers which have to be converted to \r\n",
        "    the words they correspond to.\r\n",
        "\r\n",
        "  @param:\r\n",
        "    - table: the dataframe that has been processed by genBagOfWords.\r\n",
        "  \r\n",
        "  @return:\r\n",
        "    - dictionary {index:word(s)}\r\n",
        "\r\n",
        "  \r\n",
        "  Note: The dataframe is converted to a RDD here so we can apply PySpark mapreduce\r\n",
        "        functions on it.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # column names in the dataframe\r\n",
        "  COL_TWEETS = 'tweet'\r\n",
        "  COL_HASHINGTF_RESULTS = 'hashed'\r\n",
        "  COL_FEATURES = \"features\"\r\n",
        "  tableRdd = table.rdd\r\n",
        "  \r\n",
        "  #put index and corresponding word in a list\r\n",
        "  def putWordInDict(row):\r\n",
        "    wd = []\r\n",
        "    words = row[COL_TWEETS]\r\n",
        "    indicies = row[COL_HASHINGTF_RESULTS].indices\r\n",
        "    values = row[COL_FEATURES].values\r\n",
        "    for i in range(len(indicies)):\r\n",
        "      wd.append((indicies[i],(values[i],words[i])))\r\n",
        "    return wd\r\n",
        "  \r\n",
        "  # gets rid of the same words that appear twice or more\r\n",
        "  def getRidOfDuplicateWords(wordList):\r\n",
        "    toBeReturned = []\r\n",
        "    for word in wordList:\r\n",
        "      if len(toBeReturned)==0:\r\n",
        "        toBeReturned.append(word)\r\n",
        "      if word in toBeReturned:\r\n",
        "        continue\r\n",
        "      else:\r\n",
        "        toBeReturned.append(word)\r\n",
        "    return toBeReturned\r\n",
        "      \r\n",
        "  \r\n",
        "  # sorts words by their weights\r\n",
        "  def keepHighestValueWords(kvPair):\r\n",
        "    key = kvPair[0]\r\n",
        "    valueWordTupList = list(kvPair[1])\r\n",
        "    aggDict = {}\r\n",
        "    valList = []\r\n",
        "    for vwTup in valueWordTupList:\r\n",
        "      aggDict[vwTup[0]] = vwTup[1]\r\n",
        "      valList.append(vwTup[0])\r\n",
        "    sortedVals = np.sort(np.array(valList))[::-1]\r\n",
        "    wordsList = []\r\n",
        "    for i in sortedVals:\r\n",
        "      wordsList.append(aggDict[i])\r\n",
        "    \r\n",
        "    return (key,wordsList)\r\n",
        "    \r\n",
        "\r\n",
        "  # mapreduce functions to apply the method defined above to the dataset\r\n",
        "  wordIndices = tableRdd.flatMap(putWordInDict)\\\r\n",
        "                        .sortByKey()\\\r\n",
        "                        .groupByKey()\\\r\n",
        "                        .map(keepHighestValueWords)\\\r\n",
        "                        .map(lambda x: (x[0],getRidOfDuplicateWords(x[1])))\r\n",
        "\r\n",
        "  #create an actual dictionary: Note -> this is a sequential operation and takes up the most time in this section\r\n",
        "  wordDictionary = {}\r\n",
        "  for indx in wordIndices.collect():\r\n",
        "    wordList = []\r\n",
        "    for word in indx[1]:\r\n",
        "      wordList.append(word)\r\n",
        "    wordDictionary[indx[0]] = wordList\r\n",
        "\r\n",
        "  return wordDictionary\r\n",
        "\r\n",
        "\r\n",
        "def genBagOfWords(preprocessedRdd):\r\n",
        "  r\"\"\"\r\n",
        "    Generates a bag of words model and word-index dictionary from a given data rdd.\r\n",
        "    The rdd has to be pre-processed already, so it must be the output from the\r\n",
        "    doDataPreProcessing() function.\r\n",
        "\r\n",
        "  @param: \r\n",
        "    - preprocessedRdd : an rdd that contains the pre-processed data. This rdd should be\r\n",
        "                the one obtained from txtDataRDD.map(preProcessTweet)\r\n",
        "\r\n",
        "  @return:\r\n",
        "    - tuple:\r\n",
        "        - [0] : tweets_bagOfWords_DF : dataframe\r\n",
        "        - [1] : wordDic : dictonary of indices to words\r\n",
        "            - format : {index(int):words(list)}\r\n",
        "  \r\n",
        "  @description:\r\n",
        "    This method creates a sql dataframe using the data from the rdd. Then uses\r\n",
        "    the that table with the HashingTF and IDF classes to create the bag of words\r\n",
        "    model. HashingTF and IDF can take in dataframe objects and expand them with\r\n",
        "    their outputs. From the final dataframe produced by the IDF class a dictionary\r\n",
        "    of indices with their respective words. The dictionary will be used to interpret\r\n",
        "    results later.\r\n",
        "\r\n",
        "    Steps:\r\n",
        "      - Create DataFrame that has columns index and tweet.\r\n",
        "      \r\n",
        "      - Do Term Frequency (TF) transformation using HashingTF to generate indices\r\n",
        "        for each word(we had the indices for tweets before not words). HashingTF \r\n",
        "        will take the DataFrame created in 1st step and return tweet_tf_DF which \r\n",
        "        has all of the previous dataframe data with the hashed TF indices.\r\n",
        "\r\n",
        "      - Do Inverse Document Frequency (IDF) transformation, this extracts feature\r\n",
        "        data. Note this requires two calls to the IDF class to do (see pyspark IDF\r\n",
        "        documentation). IDF takes the dataframe tweet_tf_DF produced by HashingTF\r\n",
        "        as input and outputs tweets_bagOfWords_DF. Note: Bag of Words = TF-IDF\r\n",
        "        transformation, a IDF transformation after TF transformation.\r\n",
        "\r\n",
        "      - Create Dictionary of indices to their words using the generateWordsDict()\r\n",
        "        method.\r\n",
        "\r\n",
        "      - Return results\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # spark session:\r\n",
        "  spark_session = spark.sql.SparkSession.builder.appName(\"dfexp_inmeth\").getOrCreate()\r\n",
        "\r\n",
        "  dataList = preprocessedRdd.collect()\r\n",
        "\r\n",
        "  #give each tweet/document an index:\r\n",
        "  dataListIndexed = []\r\n",
        "  for i in range(len(dataList)):\r\n",
        "    dataListIndexed.append((i,dataList[i]))\r\n",
        "  \r\n",
        "  #create dataframe with the spark session spark_sess:\r\n",
        "  tweet_DF = spark_session.createDataFrame(dataListIndexed,[\"index\",\"tweet\"])\r\n",
        "  tweetHasher = HashingTF(inputCol=\"tweet\",outputCol=\"hashed\",numFeatures=NUM_FEATURES)\r\n",
        "  tweet_tf_DF = tweetHasher.transform(tweet_DF)\r\n",
        "  idfTransformer = IDF(inputCol=\"hashed\",outputCol=\"features\")\r\n",
        "  idfFitter = idfTransformer.fit(tweet_tf_DF)\r\n",
        "  tweets_bagOfWords_DF = idfFitter.transform(tweet_tf_DF) #note bagOfWords = tf-idf transformation\r\n",
        "\r\n",
        "  #get the word dictionary:\r\n",
        "  wordDic = generateWordDict(tweets_bagOfWords_DF)\r\n",
        "\r\n",
        "  #return both the table and the word dictionary:\r\n",
        "  return (tweets_bagOfWords_DF,wordDic)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5f90zjmRQUo"
      },
      "source": [
        "# ML Algo Implementation\r\n",
        "\r\n",
        "r\"\"\"\r\n",
        "Call PySpark's KMeans algorithm, providing it the appropriate data. The results from the algorithm will be a vector of floats which will be converted to words\r\n",
        "that represent the topics. The conversion will be done through the word index\r\n",
        "dictionary created during the Feature Extraction section.\r\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZeSlSWURsT1"
      },
      "source": [
        "def getTopicWords(clusterCenterList,indexWordDictionary):\r\n",
        "  r\"\"\"\r\n",
        "    Takes the indices produced by the KMeans algorithms and matches it\r\n",
        "    with the corresponding word(s). These words will represent the\r\n",
        "    topics and themes discovered by the KMeans algorithm.\r\n",
        "\r\n",
        "  @param:\r\n",
        "    - clusterCenterList : list of cluster centers\r\n",
        "    - indexWordDictionary: dictionary that matches words with indices used by the KMenas algo; created during Feature Extraction phase.\r\n",
        "  \r\n",
        "  @return:\r\n",
        "    - list of topics now with words instead of numbers(indices)\r\n",
        "  \"\"\"\r\n",
        "  topicWordsAllCluster = []\r\n",
        "  clusterIndex = 0\r\n",
        "  for clusterCenter in clusterCenterList:\r\n",
        "    topicWordsPerCluster = []\r\n",
        "    print(\"Cluster \" + str(clusterIndex+1) + \" :\")\r\n",
        "    clusterIndex += 1\r\n",
        "\r\n",
        "    featureArray = clusterCenter\r\n",
        "    featureIndexMap = []\r\n",
        "    \r\n",
        "    for i in range(len(featureArray)):\r\n",
        "      if featureArray[i] != 0 :\r\n",
        "        featureIndexMap.append((i,featureArray[i]))\r\n",
        "    descendingFeatureWeight = sorted(featureIndexMap,key=lambda p:p[1])\r\n",
        "    descendingFeatureWeight.reverse()\r\n",
        "    top5Terms = descendingFeatureWeight[0:NUM_TOPICS_PER_CLUSTER]\r\n",
        "\r\n",
        "    for i in top5Terms:\r\n",
        "      topicWordsPerCluster.append(indexWordDictionary[i[0]])\r\n",
        "    \r\n",
        "    print(topicWordsPerCluster)\r\n",
        "    print()\r\n",
        "    topicWordsAllCluster.append((clusterIndex,topicWordsPerCluster))\r\n",
        "  return topicWordsAllCluster\r\n",
        "\r\n",
        "\r\n",
        "def doKMeans(dataFrame,indexWordDictionary):\r\n",
        "  r\"\"\"\r\n",
        "    Carries out PySpark's KMeans algorithm. This method calls getTopicWords and\r\n",
        "    to generate the words from topics and returns that result.\r\n",
        "\r\n",
        "  @param:\r\n",
        "    - dataFrame: table containing the features that will be fed into the algorithm\r\n",
        "    - indexWordDictionary: dictionary with index as key and word(s) as value\r\n",
        "  \"\"\"\r\n",
        "  kmeansFitter = KMeans(featuresCol=\"features\",k=NUM_CLUSTERS,maxIter=MAX_ITERATION)\r\n",
        "  kmeansModel = kmeansFitter.fit(dataFrame)\r\n",
        "  return getTopicWords(kmeansModel.clusterCenters(),indexWordDictionary)\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXSiRfpj0iRm"
      },
      "source": [
        "# Output Printer\r\n",
        "\r\n",
        "Method to print output to a specified output directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wan65V5B0mSZ"
      },
      "source": [
        "def writeOutput(path,topicClusters,durations):\r\n",
        "  r\"\"\"\r\n",
        "    Takes the topics clusters and duration dictionary\r\n",
        "    and print them to the file defined by path.\r\n",
        "  \"\"\"\r\n",
        "  with open(path,mode='a') as outFile:\r\n",
        "    #write the topic clusters into the file\r\n",
        "    topicsString = \"\"\r\n",
        "    for tc in topicClusters:\r\n",
        "      topicsString += \"Cluster \" + str(tc[0]) + \": \\n{\"\r\n",
        "      for wordList in tc[1]:\r\n",
        "        for word in wordList:\r\n",
        "          topicsString += word + \",\"\r\n",
        "      topicsString = topicsString[0:len(topicsString)-1]\r\n",
        "      topicsString += \"}\\n\\n\"\r\n",
        "    outFile.write(topicsString)\r\n",
        "    \r\n",
        "    #write the durations down to the file\r\n",
        "    durationString = \"\"\r\n",
        "    for d in durations:\r\n",
        "      durationString += d + \" : \" + str(durations[d]) + \", \"\r\n",
        "    durationString = durationString[0:len(durationString)-2]\r\n",
        "    outFile.write(durationString)\r\n",
        "    outFile.write(\"\\n\\t\\t\\t---------------------------------------------------------------\\n\\n\")\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y_TOlZkJJ2j"
      },
      "source": [
        "# Driver\r\n",
        "\r\n",
        "Runs the ml pipeline and prints outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UNexdRaW-Eo"
      },
      "source": [
        "def clusteringPipeline(inputPath,outputPath,numClusters=None,numMaxIterations=None,numPartitions=None):\r\n",
        "  r\"\"\"\r\n",
        "    Calls each of the methods related to the machine learning\r\n",
        "    pipeline and returns the result. If output path is not None then\r\n",
        "    it will also write the output to a file.\r\n",
        "  \"\"\"\r\n",
        "  if numClusters is not None:\r\n",
        "    NUM_CLUSTERS = numClusters\r\n",
        "  \r\n",
        "  if numMaxIterations is not None:\r\n",
        "    MAX_ITERATION = numMaxIterations\r\n",
        "\r\n",
        "  dataRdd = getTextData(inputPath)\r\n",
        "\r\n",
        "  preProcessingStartTime = time.time()\r\n",
        "  processedRdd = doDataPreProcessing(dataRdd,None)\r\n",
        "  preProcessingEndTime = time.time()\r\n",
        "  \r\n",
        "  featureExtractionStartTime = time.time()\r\n",
        "  df,wordDic = genBagOfWords(processedRdd)\r\n",
        "  featureExtractionEndTime = time.time()\r\n",
        "  \r\n",
        "  mlAlgoStartTime = time.time()\r\n",
        "  topics = doKMeans(df,wordDic)\r\n",
        "  mlAlgoEndTime = time.time()\r\n",
        "\r\n",
        "\r\n",
        "  preProcessingDuration = preProcessingEndTime - preProcessingStartTime\r\n",
        "  featureExtractionDuration = featureExtractionEndTime - featureExtractionStartTime\r\n",
        "  mlAlgoDuration = mlAlgoEndTime - mlAlgoStartTime\r\n",
        "  totalDuration = preProcessingDuration + featureExtractionDuration + mlAlgoDuration\r\n",
        "  \r\n",
        "  durationsDic = {\"PreProcessing\":preProcessingDuration,\"FeatureExtraction\":featureExtractionDuration,\"MLAlgo\":mlAlgoDuration,\"TotalDuration\":totalDuration}\r\n",
        "  print(durationsDic)\r\n",
        "\r\n",
        "  if outputPath is not None:\r\n",
        "    writeOutput(outputPath,topics,durationsDic)\r\n",
        "\r\n",
        "  return (df,wordDic,topics,durationsDic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4riRpnDxyfz-"
      },
      "source": [
        "# Run Application\r\n",
        "\r\n",
        "Here we call the clusteringPipelie to run the entire application and output the result. \r\n",
        "\r\n",
        "To run the application the only required input arguments are a path to the input\r\n",
        "text file. Please follow the example(s) below to run the code, granted everything prior to this cell has been run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIt8C5I6h_Xv",
        "outputId": "cbf95059-db30-4076-9a0f-541c605b2a86"
      },
      "source": [
        "x,y,t,dd = clusteringPipeline('./donald_trump.txt')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster 1 :\n",
            "[['support', 'geraldo'], ['great', 'america', 'work'], ['presid', 'blunt', 'economi', 'periscop'], ['thrive', 'america', 'hollywood'], ['make', 'great', 'announc'], ['tri', 'must', 'alway'], ['wonder', 'indiv', 'border', 'amp', 'great']]\n",
            "\n",
            "Cluster 2 :\n",
            "[['wonder', 'indiv', 'border', 'amp', 'great'], ['welcom', 'debat', 'presidenti'], ['tri', 'must', 'alway'], ['billion', 'countri', 'grow'], ['crime', 'pass', 'border', 'pour'], ['fake', 'berni', 'hillari'], ['neighbor', 'drug', 'fastbal', 'vote']]\n",
            "\n",
            "Cluster 3 :\n",
            "[['generos', 'win'], ['greatest', 'ago'], ['help'], ['washington'], ['thank'], ['adam'], ['unexpect']]\n",
            "\n",
            "Cluster 4 :\n",
            "[['like', 'great'], ['enjoy'], ['tonight'], ['p'], ['eastern'], ['realli', 'repres'], ['great', 'america', 'work']]\n",
            "\n",
            "Cluster 5 :\n",
            "[['get', 'florida'], ['christi', 'c'], ['appli', 'campaign', 'ivanka'], ['b', 'sent'], ['libra', 'use', 'need', 'busi'], ['realli', 'gonna'], ['c', 'murder']]\n",
            "\n",
            "{'PreProcessing': 0.0003159046173095703, 'FeatureExtraction': 28.76021385192871, 'MLAlgo': 9.373684406280518, 'TotalDuration': 38.13421416282654}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQSTcE9fNDD_",
        "outputId": "3626b637-5b0a-4450-84e0-3ec1edd97475"
      },
      "source": [
        "_,_,_,_ = clusteringPipeline('./joe_biden.txt','./joe_biden_output.txt',numMaxIterations=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster 1 :\n",
            "[['presid', 'give'], ['trump', 'unit'], ['retir', 'biden', \"vp'\"], ['safe', 'vp'], ['love', 'tax'], ['mark', 'speak'], ['peopl', 'back', 'young']]\n",
            "\n",
            "Cluster 2 :\n",
            "[['well'], ['care', 'rescu'], ['led'], ['barack', 'charact'], ['retir', 'biden', \"vp'\"], ['key', 'show', 'keep'], ['rememb', 'choic']]\n",
            "\n",
            "Cluster 3 :\n",
            "[['background'], ['administr'], ['close'], ['protect'], ['incompet', 'profoundli'], ['everi'], ['secur']]\n",
            "\n",
            "Cluster 4 :\n",
            "[['donald', 'vote', 'campaign'], ['worker', 'make', 'rule'], ['backlog', 'everi', 'almost'], ['instead', 'vote', 'vp', 'support'], ['long', 'refurbish'], ['fight', 'stay'], ['vote', 'today']]\n",
            "\n",
            "Cluster 5 :\n",
            "[['restor', 'solv'], ['attempt', 'sunday'], ['urgent'], ['never'], ['treatment', 'veteran'], ['war', 'sacr'], ['retir', 'biden', \"vp'\"]]\n",
            "\n",
            "{'PreProcessing': 0.0003025531768798828, 'FeatureExtraction': 11.328802108764648, 'MLAlgo': 8.685641050338745, 'TotalDuration': 20.014745712280273}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shFEznr5irds",
        "outputId": "013e1a9d-8de4-4535-8eaf-8c43daf8c9b0"
      },
      "source": [
        "_,_,_,_ = clusteringPipeline('./joe_biden.txt','./joe_biden_output.txt',numMaxIterations=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster 1 :\n",
            "[['presid', 'give'], ['trump', 'unit'], ['retir', 'biden', \"vp'\"], ['safe', 'vp'], ['women', 'came', 'nh'], ['donald', 'vote', 'campaign'], ['worker', 'make', 'rule']]\n",
            "\n",
            "Cluster 2 :\n",
            "[['letter', 'notic'], ['ask'], ['vp'], ['then-sen'], ['senat'], ['team', 'week'], ['ice']]\n",
            "\n",
            "Cluster 3 :\n",
            "[[\"can't\", 'econom'], ['need'], ['wh'], ['mccain'], ['info'], ['right'], ['balanc']]\n",
            "\n",
            "Cluster 4 :\n",
            "[['educ', 'white', 'student', 'health'], ['expand', 'student', 'cost'], ['capabl', 'affect', 'attend'], ['work', 'yesterday', 'would'], ['firsthand', 'get'], [\"we'r\", 'biden'], ['law', 'pay', 'rate']]\n",
            "\n",
            "Cluster 5 :\n",
            "[['gun', 'republican', 'live'], ['form', 'today'], ['epidem', 'forget'], ['quarterli', 'week'], ['issu', 'pass'], ['suppos'], ['louis', 'b', 'violenc']]\n",
            "\n",
            "{'PreProcessing': 0.0003197193145751953, 'FeatureExtraction': 11.101372957229614, 'MLAlgo': 7.724670886993408, 'TotalDuration': 18.826363563537598}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMiXcbnfxhIy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZCWqY_qIYAi"
      },
      "source": [
        "# Spark Stopper\r\n",
        "\r\n",
        "After finishing any tasks within this notebook and<br>\r\n",
        "if spark context was started (sc = SparkContext()) then<br>\r\n",
        "make sure to run the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQgLWOSUJHUw"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}