{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5-6TKQzS4zn",
        "outputId": "f94c887c-475f-432a-f2ac-82f51c0fab71"
      },
      "source": [
        "#pyspark:\r\n",
        "!pip install pyspark\r\n",
        "import pyspark as spark\r\n",
        "from pyspark import SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer,CountVectorizer\r\n",
        "from pyspark.ml.clustering import KMeans\r\n",
        "from pyspark.mllib.clustering import KMeans as KMeans_mllib\r\n",
        "\r\n",
        "\r\n",
        "#nltk imports\r\n",
        "import nltk\r\n",
        "from nltk.tokenize.casual import TweetTokenizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import time"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yidymfXHrO72"
      },
      "source": [
        "## Downloads\r\n",
        "\r\n",
        "To be Downloaded:\r\n",
        "  - Twitter Dataset\r\n",
        "  - Stopwords\r\n",
        "\r\n",
        "This is a run once only step to get the labeled twitter data and stopwords.\r\n",
        "\r\n",
        "\r\n",
        "After this manual processing is done on the json file to convert to \r\n",
        ".txt files that can be read and processed by pyspark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FeWTh1BuRxW5",
        "outputId": "f9fa93df-82cb-48a9-fb7d-c58fb20972fd"
      },
      "source": [
        "r\"\"\"\r\n",
        "twitter_samples = nltk.download('twitter_samples',download_dir='./')\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ntwitter_samples = nltk.download('twitter_samples',download_dir='./')\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNJXm7qqy_ws",
        "outputId": "cd7d3c87-9a60-4d8c-9504-0d6c349b6008"
      },
      "source": [
        "#nltk.download('stopwords',download_dir='/root/nltk_data')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bowa3jMZZDCg"
      },
      "source": [
        "with open(\"./english\") as eng:\r\n",
        "  engSet = set(eng)\r\n",
        "\r\n",
        "engSet = set(map(lambda x: x[0:len(x)-1],engSet))"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yznJ6xU1AoU0"
      },
      "source": [
        "## Spark Starter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4tSxazuOSgz"
      },
      "source": [
        "#spark context starter:\r\n",
        "sc = SparkContext(appName=\"inModuleTask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhVSJKqf_1pA"
      },
      "source": [
        "# Declaration/Initializations\r\n",
        "\r\n",
        "Below all of the modules,constants, and variables to be used <br>\r\n",
        "in a global manner across the code will be either declared or <br>\r\n",
        "initiated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLg2bxOpzmZi"
      },
      "source": [
        "#dataset variables and constants:\r\n",
        "# - path constants:\r\n",
        "POSITIVE_TWEETS_PATH = \"./positive_tweets_short.txt\"\r\n",
        "NEGATIVE_TWEETS_PATH = \"./negative_tweets_short.txt\"\r\n",
        "#POSITIVE_TWEETS_PATH = \"./positive_tweets.txt\"\r\n",
        "#NEGATIVE_TWEETS_PATH = \"./negative_tweets.txt\"\r\n",
        "\r\n",
        "\r\n",
        "# modules that will be used by the pre-processor\r\n",
        "tokenizer_tweets = TweetTokenizer(reduce_len=True,strip_handles=True)\r\n",
        "#stp_words = set(stopwords.words(\"english\"))\r\n",
        "stp_words = engSet\r\n",
        "porter = PorterStemmer()\r\n",
        "\r\n",
        "\r\n",
        "# for feature extraction:\r\n",
        "NUM_FEATURES = 1<<16 #this is the same as 2^N when 1<<N\r\n",
        "NUM_WORDS_KEEP = 3\r\n",
        "\r\n",
        "#clustering vars/constants:\r\n",
        "NUM_CLUSTERS = 5\r\n",
        "MAX_ITERATION = 30\r\n",
        "NUM_TOPICS_PER_CLUSTER = 7"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVVk2t-TZaii",
        "outputId": "201d88fa-ddad-4e05-9694-d83f1f38a91e"
      },
      "source": [
        "print(stp_words)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'such', \"couldn't\", 'haven', 'those', 'o', 'should', 'weren', 'i', 'into', 'so', 'this', 'am', 'have', 'through', 'when', 'nor', 'same', 'yourselves', 'why', 'over', 'it', 'most', 'or', 'we', 'hers', \"weren't\", 'having', \"it's\", 'in', 'him', \"shouldn't\", 'mightn', 'than', 'until', \"you're\", 'now', 'both', 'the', 'what', 'his', 'before', \"mustn't\", 'do', 'she', \"don't\", 'other', 'can', 're', \"wasn't\", \"you'll\", 'needn', \"you'd\", 'theirs', \"shan't\", 'itself', \"that'll\", 'our', 'yours', \"she's\", 'has', 'hasn', 'ma', 'he', 'be', 'herself', 'her', 'of', 'during', 'hadn', 'just', 'above', 'll', \"haven't\", 'on', 'mustn', 'doing', \"needn't\", 'didn', 'don', 'does', 'about', 'more', 'with', 'some', 've', 'them', \"isn't\", 'which', 'to', 'being', 'is', \"doesn't\", \"hadn't\", 'were', 'been', 'below', 'wasn', 'up', 'a', 'because', 'where', 'by', 'couldn', 'themselves', 'after', 'against', 'here', \"hasn't\", 'your', 'are', 'but', 'isn', 't', 'any', 'as', 'down', \"you've\", 'again', 'you', 'ourselves', 'once', 'doesn', \"mightn't\", 'they', 'if', 'myself', 'yourself', 'while', 'aren', \"didn't\", \"should've\", 'whom', 'at', 'will', 'did', 'how', 'ours', 'not', 'its', 'too', 's', 'each', 'all', 'had', 'own', 'an', 'there', 'their', 'for', \"aren't\", 'few', 'd', 'won', 'these', 'out', 'off', 'was', 'between', 'from', 'under', \"wouldn't\", 'y', 'then', 'himself', 'ain', 'me', 'and', 'wouldn', 'shan', 'no', 'm', 'shouldn', \"won't\", 'further', 'only', 'very', 'who', 'that', 'my'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJnap7trWRPr"
      },
      "source": [
        "# Gather Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMqrqB1wXTsC"
      },
      "source": [
        "def getTextData(path):\r\n",
        "  r\"\"\"\r\n",
        "    Will get the (text) file at the given path and return\r\n",
        "    an rdd.\r\n",
        "  \r\n",
        "  @param:\r\n",
        "    - path : path to the file\r\n",
        "  \r\n",
        "  @return:\r\n",
        "    - rdd : rdd of the textfile; separated by lines\r\n",
        "  \"\"\"\r\n",
        "  dataRdd = sc.textFile(path)\r\n",
        "  return dataRdd"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-o5G3UDOOIM"
      },
      "source": [
        "# Pre-Processing\r\n",
        "\r\n",
        "Steps in pre-processing:\r\n",
        "  - For each line of text (= one tweet):\r\n",
        "    - get rid of all non alphabetical characters\r\n",
        "    - lowercase all words\r\n",
        "    - get rid of stop words\r\n",
        "  - Break each line into words\r\n",
        "  - Convert each word to its root form, i.e stemming (through nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ7HPaMb_aWp"
      },
      "source": [
        "### Pre-Processing Method\r\n",
        "\r\n",
        "Will be plugged into pyspark map function for both\r\n",
        "positive and negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r52elpuWSzFh"
      },
      "source": [
        "#pre-processor method, will be the map function for pyspark:\r\n",
        "def preProcessTweet(tweet):\r\n",
        "  r\"\"\"\r\n",
        "    Does pre-processing of each tweet and returns a list\r\n",
        "    of processed words.\r\n",
        "\r\n",
        "    NOTE: Each line in input doc has to be a tweet for this to work.\r\n",
        "\r\n",
        "    Steps taken by this method:\r\n",
        "      - Retains only words in the tweets.\r\n",
        "      - Gets rid of stop words\r\n",
        "      - Stems each word using nltk library functions\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  #strip begining and ending whitespaces:\r\n",
        "  tweet = tweet.strip()\r\n",
        "\r\n",
        "  #tokenize the words based on nltk twitter tokenization:\r\n",
        "  tokenized_tweet = tokenizer_tweets.tokenize(tweet)\r\n",
        "\r\n",
        "  #keep only the words:\r\n",
        "  wordRegex = r'^[a-zA-Z].*$'\r\n",
        "  tokenized_tweet_onlyWords = []\r\n",
        "  for s in tokenized_tweet:\r\n",
        "    result = re.search(wordRegex,s)\r\n",
        "    if result is not None:\r\n",
        "      tokenized_tweet_onlyWords.append(result.string.lower())\r\n",
        "  \r\n",
        "  #get rid of links (NOTE: this is an approximation and may not cover everything):\r\n",
        "  tokenized_tweet_onlyWords_noLinks = []\r\n",
        "  linkRemovalRegex = r\"^https?:.*$|^.*\\..*$|^.*[@\\.].*$\"\r\n",
        "  for s in tokenized_tweet_onlyWords:\r\n",
        "    result = re.search(linkRemovalRegex,s)\r\n",
        "    if result is None:\r\n",
        "      tokenized_tweet_onlyWords_noLinks.append(s)\r\n",
        "\r\n",
        "  #get rid of stopwords:\r\n",
        "  tokenized_tweet_onlyWords_noStopwords = []\r\n",
        "  for word in tokenized_tweet_onlyWords_noLinks:\r\n",
        "    if word not in stp_words:\r\n",
        "      tokenized_tweet_onlyWords_noStopwords.append(word)\r\n",
        "  \r\n",
        "  #stem the word:\r\n",
        "  tokenizedTweet_onlyWords_noStopWords_stemmed = []\r\n",
        "  for word in tokenized_tweet_onlyWords_noStopwords:\r\n",
        "    tokenizedTweet_onlyWords_noStopWords_stemmed.append(porter.stem(word))\r\n",
        "  \r\n",
        "  \r\n",
        "  toBeReturned = tokenizedTweet_onlyWords_noStopWords_stemmed\r\n",
        "\r\n",
        "  #change all remaining words to lowercase:\r\n",
        "  for i in range(len(toBeReturned)):\r\n",
        "    toBeReturned[i] = toBeReturned[i].lower()\r\n",
        "\r\n",
        "  return toBeReturned \r\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNNkBpNLcJE3"
      },
      "source": [
        "## Pre-Processing Abstraction\r\n",
        "\r\n",
        "A method that calls the pre-processor and returns\r\n",
        "the pre-processed rdd. This is only for the sake of cleaner code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5JBBSe1cToI"
      },
      "source": [
        "def doDataPreProcessing(dataRdd,numPartitions):\r\n",
        "  r\"\"\"\r\n",
        "    Given the rdd of a textfile, this method will\r\n",
        "    return an rdd with pre-processed data. The\r\n",
        "    pre-processing will take place using pyspark map function with \r\n",
        "    the preProcessTweet() method.\r\n",
        "\r\n",
        "  @param:\r\n",
        "    - dataRdd: rdd that contains the entire text data. This is the\r\n",
        "               rdd generated by getTextData().\r\n",
        "  \r\n",
        "  @return:\r\n",
        "    - processedRdd: rdd that contains processed tweets. The format for\r\n",
        "                    each entry is a list of words relevant for each tweet.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  if numPartitions is None:\r\n",
        "    return dataRdd.map(preProcessTweet)\r\n",
        "  \r\n",
        "  return dataRdd.map(preProcessTweet)\\\r\n",
        "                .partitionBy(numPartitions)\\\r\n",
        "                .persist()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TMCzAXR-l0o"
      },
      "source": [
        "# Feature Extraction\r\n",
        "\r\n",
        "Generate Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYa3ev3y-lTh"
      },
      "source": [
        "\r\n",
        "def generateWordDict(table):\r\n",
        "  COL_TWEETS = 'tweet'\r\n",
        "  COL_HASHINGTF_RESULTS = 'hashed'\r\n",
        "  COL_FEATURES = \"features\"\r\n",
        "  tableRdd = table.rdd\r\n",
        "  \r\n",
        "  def putWordInDict(row):\r\n",
        "    wd = []\r\n",
        "    words = row[COL_TWEETS]\r\n",
        "    indicies = row[COL_HASHINGTF_RESULTS].indices\r\n",
        "    values = row[COL_FEATURES].values\r\n",
        "    for i in range(len(indicies)):\r\n",
        "      wd.append((indicies[i],(values[i],words[i])))\r\n",
        "    return wd\r\n",
        "  \r\n",
        "  def getRidOfDuplicateWords(wordList):\r\n",
        "    toBeReturned = []\r\n",
        "    for word in wordList:\r\n",
        "      if len(toBeReturned)==0:\r\n",
        "        toBeReturned.append(word)\r\n",
        "      if word in toBeReturned:\r\n",
        "        continue\r\n",
        "      else:\r\n",
        "        toBeReturned.append(word)\r\n",
        "    return toBeReturned\r\n",
        "      \r\n",
        "  \r\n",
        "  def keepHighestValueWords(kvPair):\r\n",
        "    key = kvPair[0]\r\n",
        "    valueWordTupList = list(kvPair[1])\r\n",
        "    aggDict = {}\r\n",
        "    valList = []\r\n",
        "    for vwTup in valueWordTupList:\r\n",
        "      aggDict[vwTup[0]] = vwTup[1]\r\n",
        "      valList.append(vwTup[0])\r\n",
        "    sortedVals = np.sort(np.array(valList))[::-1]\r\n",
        "    wordsList = []\r\n",
        "    for i in sortedVals:\r\n",
        "      wordsList.append(aggDict[i])\r\n",
        "    \r\n",
        "    return (key,wordsList)\r\n",
        "    \r\n",
        "\r\n",
        "  wordIndices = tableRdd.flatMap(putWordInDict)\\\r\n",
        "                        .sortByKey()\\\r\n",
        "                        .groupByKey()\\\r\n",
        "                        .map(keepHighestValueWords)\\\r\n",
        "                        .map(lambda x: (x[0],getRidOfDuplicateWords(x[1])))\r\n",
        "\r\n",
        "  #create an actual dictionary:\r\n",
        "  wordDictionary = {}\r\n",
        "  for indx in wordIndices.collect():\r\n",
        "    wordList = []\r\n",
        "    for word in indx[1]:\r\n",
        "      wordList.append(word)\r\n",
        "    wordDictionary[indx[0]] = wordList\r\n",
        "\r\n",
        "  return wordDictionary\r\n",
        "\r\n",
        "\r\n",
        "def genBagOfWords(preprocessedRdd):\r\n",
        "  r\"\"\"\r\n",
        "    Generates a bag of words model and word-index dictionary from a given data rdd.\r\n",
        "    The rdd has to be pre-processed already, so it must be the output from the\r\n",
        "    doDataPreProcessing() function.\r\n",
        "\r\n",
        "  @param: \r\n",
        "    - preprocessedRdd : an rdd that contains the pre-processed data. This rdd should be\r\n",
        "                the one obtained from txtDataRDD.map(preProcessTweet)\r\n",
        "\r\n",
        "  @return:\r\n",
        "    - tuple:\r\n",
        "        - [0] : tweets_bagOfWords_DF : dataframe\r\n",
        "        - [1] : wordDic : dictonary of indices to words\r\n",
        "            - format : {index(int):words(list)}\r\n",
        "  \r\n",
        "  @description:\r\n",
        "    This method creates a sql dataframe using the data from the rdd. Then uses\r\n",
        "    the that table with the HashingTF and IDF classes to create the bag of words\r\n",
        "    model. HashingTF and IDF can take in dataframe objects and expand them with\r\n",
        "    their outputs. From the final dataframe produced by the IDF class a dictionary\r\n",
        "    of indices with their respective words. The dictionary will be used to interpret\r\n",
        "    results later.\r\n",
        "\r\n",
        "    Steps:\r\n",
        "      - Create DataFrame that has columns index and tweet.\r\n",
        "      \r\n",
        "      - Do Term Frequency (TF) transformation using HashingTF to generate indices\r\n",
        "        for each word(we had the indices for tweets before not words). HashingTF \r\n",
        "        will take the DataFrame created in 1st step and return tweet_tf_DF which \r\n",
        "        has all of the previous dataframe data with the hashed TF indices.\r\n",
        "\r\n",
        "      - Do Inverse Document Frequency (IDF) transformation, this extracts feature\r\n",
        "        data. Note this requires two calls to the IDF class to do (see pyspark IDF\r\n",
        "        documentation). IDF takes the dataframe tweet_tf_DF produced by HashingTF\r\n",
        "        as input and outputs tweets_bagOfWords_DF. Note: Bag of Words = TF-IDF\r\n",
        "        transformation, a IDF transformation after TF transformation.\r\n",
        "\r\n",
        "      - Create Dictionary of indices to their words using the generateWordsDict()\r\n",
        "        method.\r\n",
        "\r\n",
        "      - Return results\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # spark session:\r\n",
        "  spark_session = spark.sql.SparkSession.builder.appName(\"dfexp_inmeth\").getOrCreate()\r\n",
        "\r\n",
        "  dataList = preprocessedRdd.collect()\r\n",
        "\r\n",
        "  #give each tweet/document an index:\r\n",
        "  dataListIndexed = []\r\n",
        "  for i in range(len(dataList)):\r\n",
        "    dataListIndexed.append((i,dataList[i]))\r\n",
        "  \r\n",
        "  #create dataframe with the spark session spark_sess:\r\n",
        "  tweet_DF = spark_session.createDataFrame(dataListIndexed,[\"index\",\"tweet\"])\r\n",
        "  tweetHasher = HashingTF(inputCol=\"tweet\",outputCol=\"hashed\",numFeatures=NUM_FEATURES)\r\n",
        "  #cv = CountVectorizer(inputCol=\"tweet\",outputCol=\"hashed\")\r\n",
        "  tweet_tf_DF = tweetHasher.transform(tweet_DF)\r\n",
        "  #tweet_tf_DF = cv.fit(tweet_DF)\r\n",
        "  #tweet_tf_DF = tweet_tf_DF.transform(tweet_DF)\r\n",
        "  idfTransformer = IDF(inputCol=\"hashed\",outputCol=\"features\")\r\n",
        "  idfFitter = idfTransformer.fit(tweet_tf_DF)\r\n",
        "  tweets_bagOfWords_DF = idfFitter.transform(tweet_tf_DF) #note bagOfWords = tf-idf transformation\r\n",
        "\r\n",
        "  #get the word dictionary:\r\n",
        "  wordDic = generateWordDict(tweets_bagOfWords_DF)\r\n",
        "\r\n",
        "  #return both the table and the word dictionary:\r\n",
        "  return (tweets_bagOfWords_DF,wordDic)\r\n",
        "\r\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5f90zjmRQUo"
      },
      "source": [
        "# ML Algo Implementation\r\n",
        "\r\n",
        "Call PySpark's KMeans algorithm, providing it the appropriate data. The results from the algorithm will be a vector of floats which will be converted to words\r\n",
        "that represent the topics. The conversion will be done through the word index\r\n",
        "dictionary created during the Feature Extraction section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZeSlSWURsT1"
      },
      "source": [
        "def getTopicWords(clusterCenterList,indexWordDictionary):\r\n",
        "  topicWordsAllCluster = []\r\n",
        "  clusterIndex = 0\r\n",
        "  for clusterCenter in clusterCenterList:\r\n",
        "    topicWordsPerCluster = []\r\n",
        "    print(\"Cluster \" + str(clusterIndex+1) + \" :\")\r\n",
        "    clusterIndex += 1\r\n",
        "\r\n",
        "    featureArray = clusterCenter\r\n",
        "    featureIndexMap = []\r\n",
        "    \r\n",
        "    for i in range(len(featureArray)):\r\n",
        "      if featureArray[i] != 0 :\r\n",
        "        featureIndexMap.append((i,featureArray[i]))\r\n",
        "    descendingFeatureWeight = sorted(featureIndexMap,key=lambda p:p[1])\r\n",
        "    descendingFeatureWeight.reverse()\r\n",
        "    top5Terms = descendingFeatureWeight[0:NUM_TOPICS_PER_CLUSTER]\r\n",
        "\r\n",
        "    for i in top5Terms:\r\n",
        "      topicWordsPerCluster.append(indexWordDictionary[i[0]])\r\n",
        "    \r\n",
        "    print(topicWordsPerCluster)\r\n",
        "    print()\r\n",
        "    topicWordsAllCluster.append((clusterIndex,topicWordsPerCluster))\r\n",
        "  return topicWordsAllCluster\r\n",
        "\r\n",
        "def doKMeans(dataFrame,indexWordDictionary):\r\n",
        "  kmeansFitter = KMeans(featuresCol=\"features\",k=NUM_CLUSTERS,maxIter=MAX_ITERATION)\r\n",
        "  kmeansModel = kmeansFitter.fit(dataFrame)\r\n",
        "  return getTopicWords(kmeansModel.clusterCenters(),indexWordDictionary)\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXSiRfpj0iRm"
      },
      "source": [
        "# Output Printer\r\n",
        "\r\n",
        "Method to print output to a specified output directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wan65V5B0mSZ"
      },
      "source": [
        "def writeOutput(path,topicClusters,durations):\r\n",
        "  with open(path,mode='a') as outFile:\r\n",
        "    #write the topic clusters into the file\r\n",
        "    topicsString = \"\"\r\n",
        "    for tc in topicClusters:\r\n",
        "      topicsString += \"Cluster \" + str(tc[0]) + \": \\n{\"\r\n",
        "      for wordList in tc[1]:\r\n",
        "        for word in wordList:\r\n",
        "          topicsString += word + \",\"\r\n",
        "      topicsString = topicsString[0:len(topicsString)-1]\r\n",
        "      topicsString += \"}\\n\\n\"\r\n",
        "    outFile.write(topicsString)\r\n",
        "    \r\n",
        "    #write the durations down to the file\r\n",
        "    durationString = \"\"\r\n",
        "    for d in durations:\r\n",
        "      durationString += d + \" : \" + str(durations[d]) + \", \"\r\n",
        "    durationString = durationString[0:len(durationString)-2]\r\n",
        "    outFile.write(durationString)\r\n",
        "    outFile.write(\"\\n\\t\\t\\t---------------------------------------------------------------\\n\\n\")\r\n",
        "  "
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y_TOlZkJJ2j"
      },
      "source": [
        "# Driver\r\n",
        "\r\n",
        "Runs the ml pipeline and prints outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UNexdRaW-Eo"
      },
      "source": [
        "def clusteringPipeline(inputPath,outputPath,numClusters=None,numMaxIterations=None,numPartitions=None):\r\n",
        "  if numClusters is not None:\r\n",
        "    NUM_CLUSTERS = numClusters\r\n",
        "  \r\n",
        "  if numMaxIterations is not None:\r\n",
        "    MAX_ITERATION = numMaxIterations\r\n",
        "\r\n",
        "  dataRdd = getTextData(inputPath)\r\n",
        "\r\n",
        "  preProcessingStartTime = time.time()\r\n",
        "  processedRdd = doDataPreProcessing(dataRdd,None)\r\n",
        "  preProcessingEndTime = time.time()\r\n",
        "  \r\n",
        "  featureExtractionStartTime = time.time()\r\n",
        "  df,wordDic = genBagOfWords(processedRdd)\r\n",
        "  featureExtractionEndTime = time.time()\r\n",
        "  \r\n",
        "  mlAlgoStartTime = time.time()\r\n",
        "  topics = doKMeans(df,wordDic)\r\n",
        "  mlAlgoEndTime = time.time()\r\n",
        "\r\n",
        "\r\n",
        "  preProcessingDuration = preProcessingEndTime - preProcessingStartTime\r\n",
        "  featureExtractionDuration = featureExtractionEndTime - featureExtractionStartTime\r\n",
        "  mlAlgoDuration = mlAlgoEndTime - mlAlgoStartTime\r\n",
        "  totalDuration = preProcessingDuration + featureExtractionDuration + mlAlgoDuration\r\n",
        "  \r\n",
        "  durationsDic = {\"PreProcessing\":preProcessingDuration,\"FeatureExtraction\":featureExtractionDuration,\"MLAlgo\":mlAlgoDuration,\"TotalDuration\":totalDuration}\r\n",
        "  print(durationsDic)\r\n",
        "\r\n",
        "  writeOutput(outputPath,t,dd)\r\n",
        "\r\n",
        "  return (df,wordDic,topics,durationsDic)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIt8C5I6h_Xv",
        "outputId": "cbf95059-db30-4076-9a0f-541c605b2a86"
      },
      "source": [
        "x,y,t,dd = clusteringPipeline('./donald_trump.txt',\"./donald_trump_output.txt\",3,50)\r\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster 1 :\n",
            "[['support', 'geraldo'], ['great', 'america', 'work'], ['presid', 'blunt', 'economi', 'periscop'], ['thrive', 'america', 'hollywood'], ['make', 'great', 'announc'], ['tri', 'must', 'alway'], ['wonder', 'indiv', 'border', 'amp', 'great']]\n",
            "\n",
            "Cluster 2 :\n",
            "[['wonder', 'indiv', 'border', 'amp', 'great'], ['welcom', 'debat', 'presidenti'], ['tri', 'must', 'alway'], ['billion', 'countri', 'grow'], ['crime', 'pass', 'border', 'pour'], ['fake', 'berni', 'hillari'], ['neighbor', 'drug', 'fastbal', 'vote']]\n",
            "\n",
            "Cluster 3 :\n",
            "[['generos', 'win'], ['greatest', 'ago'], ['help'], ['washington'], ['thank'], ['adam'], ['unexpect']]\n",
            "\n",
            "Cluster 4 :\n",
            "[['like', 'great'], ['enjoy'], ['tonight'], ['p'], ['eastern'], ['realli', 'repres'], ['great', 'america', 'work']]\n",
            "\n",
            "Cluster 5 :\n",
            "[['get', 'florida'], ['christi', 'c'], ['appli', 'campaign', 'ivanka'], ['b', 'sent'], ['libra', 'use', 'need', 'busi'], ['realli', 'gonna'], ['c', 'murder']]\n",
            "\n",
            "{'PreProcessing': 0.0003159046173095703, 'FeatureExtraction': 28.76021385192871, 'MLAlgo': 9.373684406280518, 'TotalDuration': 38.13421416282654}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQSTcE9fNDD_",
        "outputId": "3626b637-5b0a-4450-84e0-3ec1edd97475"
      },
      "source": [
        "_,_,_,_ = clusteringPipeline('./joe_biden.txt','./joe_biden_output.txt',numMaxIterations=50)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster 1 :\n",
            "[['presid', 'give'], ['trump', 'unit'], ['retir', 'biden', \"vp'\"], ['safe', 'vp'], ['love', 'tax'], ['mark', 'speak'], ['peopl', 'back', 'young']]\n",
            "\n",
            "Cluster 2 :\n",
            "[['well'], ['care', 'rescu'], ['led'], ['barack', 'charact'], ['retir', 'biden', \"vp'\"], ['key', 'show', 'keep'], ['rememb', 'choic']]\n",
            "\n",
            "Cluster 3 :\n",
            "[['background'], ['administr'], ['close'], ['protect'], ['incompet', 'profoundli'], ['everi'], ['secur']]\n",
            "\n",
            "Cluster 4 :\n",
            "[['donald', 'vote', 'campaign'], ['worker', 'make', 'rule'], ['backlog', 'everi', 'almost'], ['instead', 'vote', 'vp', 'support'], ['long', 'refurbish'], ['fight', 'stay'], ['vote', 'today']]\n",
            "\n",
            "Cluster 5 :\n",
            "[['restor', 'solv'], ['attempt', 'sunday'], ['urgent'], ['never'], ['treatment', 'veteran'], ['war', 'sacr'], ['retir', 'biden', \"vp'\"]]\n",
            "\n",
            "{'PreProcessing': 0.0003025531768798828, 'FeatureExtraction': 11.328802108764648, 'MLAlgo': 8.685641050338745, 'TotalDuration': 20.014745712280273}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZCWqY_qIYAi"
      },
      "source": [
        "# Spark Stopper\r\n",
        "\r\n",
        "After finishing any tasks within this notebook and<br>\r\n",
        "if spark context was started (sc = SparkContext()) then<br>\r\n",
        "make sure to run the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQgLWOSUJHUw"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": 108,
      "outputs": []
    }
  ]
}